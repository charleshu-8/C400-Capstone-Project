{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of files we want to process into single CSV data set\n",
    "files = ['github.csv', 'google.csv', 'reuters.csv', 'wikipedia.csv', 'youtube.csv']\n",
    "# List of features we want to extract per file\n",
    "features = ['Length']\n",
    "# Mapping of website names to integers to simplify output prediction for predictive models\n",
    "siteMappings = {\n",
    "    'github.csv': 0,\n",
    "    'google.csv': 1,\n",
    "    'reuters.csv': 2,\n",
    "    'wikipedia.csv': 3,\n",
    "    'youtube.csv': 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process and combine all the raw data files into singular sanitized CSV containing desired features\n",
    "\n",
    "finalDataSet = pd.DataFrame()\n",
    "\n",
    "# For every file that we have in raw_data subdirectory\n",
    "for file in files:\n",
    "    # Obtain the raw data for that particular data set\n",
    "    raw = pd.read_csv('raw_data/{}'.format(file))\n",
    "\n",
    "    # Create a new dataframe containing name of file as target\n",
    "    tempDataSet = pd.DataFrame({'Website': ['{}'.format(file)] * len(raw)})\n",
    "\n",
    "    # Extract the desired features from the raw dataset and add it to the dataframe\n",
    "    for feature in features:\n",
    "        tempDataSet = pd.concat([tempDataSet, raw[feature].to_frame()], axis=1)\n",
    "\n",
    "    # Add the sanitized data set to the final data set\n",
    "    finalDataSet = pd.concat([finalDataSet, tempDataSet]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOOD: All observations accounted for\n",
      "GOOD: All observation values correct\n"
     ]
    }
   ],
   "source": [
    "# Perform an integrity check to see if all observations were carried over\n",
    "\n",
    "rowCountPassed = False\n",
    "rowValuesPassed = True\n",
    "\n",
    "#=================\n",
    "# Row count\n",
    "#=================\n",
    "rowCount = 0\n",
    "\n",
    "# Count\n",
    "for file in files:\n",
    "    rowCount += len(pd.read_csv('raw_data/{}'.format(file)))\n",
    "\n",
    "if (len(finalDataSet) == rowCount):\n",
    "    rowCountPassed = True\n",
    "\n",
    "#=================\n",
    "# Row values\n",
    "#=================\n",
    "# First, need to check if previous test passed\n",
    "# Will not work otherwise\n",
    "if (rowCountPassed):\n",
    "    index = 0\n",
    "    # Run through every row in finalDataSet and for each file\n",
    "    for file in files:\n",
    "        raw = pd.read_csv('raw_data/{}'.format(file))\n",
    "        # We run through the raw data set and compare the values for each row to finalDataSet\n",
    "        for subindex in range(len(raw)):\n",
    "            # Check website name\n",
    "            if (finalDataSet.iloc(0)[index].get('Website') != file):\n",
    "                rowValuesPassed = False\n",
    "\n",
    "            # Check features\n",
    "            for feature in features:\n",
    "                if (finalDataSet.iloc(0)[index].get(feature) != raw.iloc(0)[subindex].get(feature)):\n",
    "                    rowValuesPassed = False\n",
    "            index += 1\n",
    "\n",
    "# Final reporting of integrity check status\n",
    "if (rowCountPassed):\n",
    "    print(\"GOOD: All observations accounted for\")\n",
    "else:\n",
    "    print(\"BAD: Missing some observations\")\n",
    "if (rowValuesPassed):\n",
    "    print(\"GOOD: All observation values correct\")\n",
    "else:\n",
    "    print(\"BAD: Some observation values incorrect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the raw sanitized data set\n",
    "finalDataSet.to_csv(\"./sanitized_data/sanitized_data_{}_unmapped.csv\".format(str(datetime.now()).replace(\" \", \"_\").replace(\".\", \"-\").replace(\":\", \"-\")))\n",
    "\n",
    "# Perform a mapping on the `Website` target column for ease of use in predictive models\n",
    "finalDataSet['Website'] = finalDataSet['Website'].map(siteMappings)\n",
    "# Save this as usable data set for models\n",
    "finalDataSet.to_csv(\"./sanitized_data/sanitized_data_{}_mapped.csv\".format(str(datetime.now()).replace(\" \", \"_\").replace(\".\", \"-\").replace(\":\", \"-\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
