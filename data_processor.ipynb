{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of files we want to process into single CSV data set\n",
    "files = [\"github.csv\", \"google.csv\", \"reuters.csv\", \"wikipedia.csv\", \"youtube.csv\"]\n",
    "# List of features we want to extract per file\n",
    "features = [\"Time\", \"Length\", \"Protocol\", \"Info\"]\n",
    "# Mapping of string values to integers to simplify output prediction for predictive models\n",
    "siteMappings = {\n",
    "    \"github.csv\": 0,\n",
    "    \"google.csv\": 1,\n",
    "    \"reuters.csv\": 2,\n",
    "    \"wikipedia.csv\": 3,\n",
    "    \"youtube.csv\": 4,\n",
    "}\n",
    "protocolMappings = {\n",
    "    \"TLSv1.3\": 0,\n",
    "    \"TCP\": 1,\n",
    "}\n",
    "infoMappings = {\n",
    "    \"[PSH, ACK]\": 0,\n",
    "    \"[SYN, ACK]\": 1,\n",
    "    \"[FIN, ACK]\": 2,\n",
    "    \"[FIN]\": 3,\n",
    "    \"[SYN]\": 4,\n",
    "    \"[PSH]\": 5,\n",
    "    \"[ACK]\": 6,\n",
    "    \"[RST]\": 7,\n",
    "    \"Server Hello, Change Cipher Spec, Application Data\": 8,\n",
    "    \"Change Cipher Spec, Application Data\": 9,\n",
    "    \"Application Data, Application Data, Application Data\": 10,\n",
    "    \"Application Data, Application Data\": 11,\n",
    "    \"Client Hello\": 12,\n",
    "    \"Server Hello\": 13,\n",
    "    \"Change Cipher Spec\": 14,\n",
    "    \"Application Data\": 15,\n",
    "    \"Continuation Data\": 16,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process and combine all the raw data files into singular sanitized CSV containing desired features\n",
    "\n",
    "finalDataSet = pd.DataFrame()\n",
    "\n",
    "# For every file that we have in raw_data subdirectory\n",
    "for file in files:\n",
    "    # Obtain the raw data for that particular data set\n",
    "    raw = pd.read_csv(\"raw_data/{}\".format(file))\n",
    "\n",
    "    # Create a new dataframe containing name of file as target\n",
    "    tempDataSet = pd.DataFrame({\"Website\": [\"{}\".format(file)] * len(raw)})\n",
    "\n",
    "    # Extract the desired features from the raw dataset and add it to the dataframe\n",
    "    for feature in features:\n",
    "        tempDataSet = pd.concat([tempDataSet, raw[feature].to_frame()], axis=1)\n",
    "\n",
    "    # Add the sanitized data set to the final data set\n",
    "    finalDataSet = pd.concat([finalDataSet, tempDataSet]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOOD: All observations accounted for\n",
      "GOOD: All observation values correct\n"
     ]
    }
   ],
   "source": [
    "# Perform an integrity check to see if all observations were carried over\n",
    "\n",
    "rowCountPassed = False\n",
    "rowValuesPassed = True\n",
    "\n",
    "#=================\n",
    "# Row count\n",
    "#=================\n",
    "rowCount = 0\n",
    "\n",
    "# Count\n",
    "for file in files:\n",
    "    rowCount += len(pd.read_csv(\"raw_data/{}\".format(file)))\n",
    "\n",
    "if (len(finalDataSet) == rowCount):\n",
    "    rowCountPassed = True\n",
    "\n",
    "#=================\n",
    "# Row values\n",
    "#=================\n",
    "# First, need to check if previous test passed\n",
    "# Will not work otherwise\n",
    "if (rowCountPassed):\n",
    "    index = 0\n",
    "    # Run through every row in finalDataSet and for each file\n",
    "    for file in files:\n",
    "        raw = pd.read_csv(\"raw_data/{}\".format(file))\n",
    "        # We run through the raw data set and compare the values for each row to finalDataSet\n",
    "        for subindex in range(len(raw)):\n",
    "            # Check website name\n",
    "            if (finalDataSet.iloc(0)[index].get(\"Website\") != file):\n",
    "                rowValuesPassed = False\n",
    "\n",
    "            # Check features\n",
    "            for feature in features:\n",
    "                if (finalDataSet.iloc(0)[index].get(feature) != raw.iloc(0)[subindex].get(feature)):\n",
    "                    rowValuesPassed = False\n",
    "            index += 1\n",
    "\n",
    "# Final reporting of integrity check status\n",
    "if (rowCountPassed):\n",
    "    print(\"GOOD: All observations accounted for\")\n",
    "else:\n",
    "    print(\"BAD: Missing some observations\")\n",
    "if (rowValuesPassed):\n",
    "    print(\"GOOD: All observation values correct\")\n",
    "else:\n",
    "    print(\"BAD: Some observation values incorrect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specialized mapper function to taret strings that contain a key rather than being the key\n",
    "def mapValues(x):\n",
    "    final = \"N/A\"\n",
    "    for key in infoMappings:\n",
    "        if key in x:\n",
    "            final = infoMappings.get(key)\n",
    "            break\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the raw sanitized data set\n",
    "finalDataSet.to_csv(\"./sanitized_data/sanitized_data_{}_unmapped.csv\".format(str(datetime.now()).replace(\" \", \"_\").replace(\".\", \"-\").replace(\":\", \"-\")), index=False)\n",
    "\n",
    "# Perform a mapping on the `Website` target column for ease of use in predictive models\n",
    "finalDataSet[\"Website\"] = finalDataSet[\"Website\"].map(siteMappings)\n",
    "finalDataSet[\"Protocol\"] = finalDataSet[\"Protocol\"].map(protocolMappings)\n",
    "finalDataSet[\"Info\"] = finalDataSet[\"Info\"].map(mapValues)\n",
    "# Save this as usable data set for models\n",
    "finalDataSet.to_csv(\"./sanitized_data/sanitized_data_{}_mapped.csv\".format(str(datetime.now()).replace(\" \", \"_\").replace(\".\", \"-\").replace(\":\", \"-\")), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
